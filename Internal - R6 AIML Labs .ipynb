{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Questions - Internal - R6 AIML Labs .ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zUZjPnVXGz0Z","colab_type":"text"},"source":["# The Iris Dataset\n","The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n","\n","The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."]},{"cell_type":"markdown","metadata":{"id":"RMbmpriavLE9","colab_type":"text"},"source":["### Specifying the TensorFlow version\n","Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fu8bUU__oa7h","colab":{}},"source":["#%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bLz1Ckvfvn6D"},"source":["### Import TensorFlow\n","Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CWrzVTLOvn6M","outputId":"16b69538-1db7-455c-e559-da599772ee35","executionInfo":{"status":"ok","timestamp":1580289954032,"user_tz":-330,"elapsed":2685,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["1.15.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_uYeJgkNuXNC","colab_type":"text"},"source":["### Set random seed"]},{"cell_type":"code","metadata":{"id":"lcASNsewsfQX","colab_type":"code","outputId":"5522f83c-4b17-4960-c4f4-6a83442d29fc","executionInfo":{"status":"error","timestamp":1580289976786,"user_tz":-330,"elapsed":1093,"user":{"displayName":"Chandan Singh","photoUrl":"","userId":"03587266437167142171"}},"colab":{"base_uri":"https://localhost:8080/","height":282}},"source":["tf.random.set_seed(42)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-ed2629a0dcab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.random' has no attribute 'set_seed'"]}]},{"cell_type":"markdown","metadata":{"id":"5-vVQBBqg7DI","colab_type":"text"},"source":["## Question 1"]},{"cell_type":"markdown","metadata":{"id":"kE0EDKvQhEIe","colab_type":"text"},"source":["### Import dataset\n","- Import iris dataset\n","- Import the dataset using sklearn library"]},{"cell_type":"code","metadata":{"id":"IOOWpD26Haq3","colab_type":"code","colab":{}},"source":["# Import necessary packages\n","import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hBCMAks4USXC","colab_type":"code","colab":{}},"source":["from sklearn.datasets import load_iris\n","iris_df = load_iris()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgenDvRWcXAd","colab_type":"code","colab":{}},"source":["#import pandas as pd\n","#import io\n","#iris_data= pd.read_csv(io.StringIO(uploaded['iris.csv'].decode('utf-8')))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcVk3ZpuVZPU","colab_type":"code","outputId":"142f8916-9653-44c7-9198-3950d15e42c8","executionInfo":{"status":"ok","timestamp":1580285717836,"user_tz":-330,"elapsed":822,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["list(iris_df.target_names)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['setosa', 'versicolor', 'virginica']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"ta8YqInTh5v5","colab_type":"text"},"source":["## Question 2"]},{"cell_type":"markdown","metadata":{"id":"HERt3drbhX0i","colab_type":"text"},"source":["### Get features and label from the dataset in separate variable\n","- you can get the features using .data method\n","- you can get the features using .target method"]},{"cell_type":"code","metadata":{"id":"dB8sJTqqVMU7","colab_type":"code","colab":{}},"source":["iris_features = iris_df.data\n","iris_target = iris_df.target"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qg1A2lkUjFak","colab_type":"text"},"source":["## Question 3"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3YErwYLCH0N_"},"source":["### Create train and test data\n","- use train_test_split to get train and test set\n","- set a random_state\n","- test_size: 0.25"]},{"cell_type":"code","metadata":{"id":"TYKNJL85h7pQ","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(iris_features, iris_target, test_size=0.25, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g0KVP17Ozaix","colab_type":"text"},"source":["## Question 4"]},{"cell_type":"markdown","metadata":{"id":"SIjqxbhWv1zv","colab_type":"text"},"source":["### One-hot encode the labels\n","- convert class vectors (integers) to binary class matrix\n","- convert labels\n","- number of classes: 3\n","- we are doing this to use categorical_crossentropy as loss"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R9vv-_gpyLY9","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","y_train=to_categorical(y_train,3)\n","y_test=to_categorical(y_test,3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ovjLyYzWkO9s"},"source":["## Question 5"]},{"cell_type":"markdown","metadata":{"id":"hbIFzoPNSyYo","colab_type":"text"},"source":["### Initialize a sequential model\n","- Define a sequential model"]},{"cell_type":"code","metadata":{"id":"4FvSbf1UjHtl","colab_type":"code","outputId":"763540ab-dd98-4c31-e9f1-a0db25ca52fd","executionInfo":{"status":"ok","timestamp":1580285750239,"user_tz":-330,"elapsed":793,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.models import Sequential\n","from keras.layers.core import Dense,Dropout,Flatten,Activation\n","from sklearn import metrics"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OD8R5e6FXdlx","colab_type":"code","outputId":"ad4cca3a-de5d-4e55-bd36-44f4c961bfdd","executionInfo":{"status":"ok","timestamp":1580285753281,"user_tz":-330,"elapsed":778,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["#Initialize the Sequence\n","model = Sequential()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dGMy999vlacX"},"source":["## Question 6"]},{"cell_type":"markdown","metadata":{"id":"72ibK5Jxm8iL","colab_type":"text"},"source":["### Add a layer\n","- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n","- Apply Softmax on Dense Layer outputs"]},{"cell_type":"code","metadata":{"id":"uZKrBNSRm_o9","colab_type":"code","outputId":"80d918a6-5675-4a9e-af6a-a868a10d91f8","executionInfo":{"status":"ok","timestamp":1580285766814,"user_tz":-330,"elapsed":827,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["# Add an input layer\n","model.add(Dense(12,input_shape=(4,),activation='softmax'))\n","#Add an output layer\n","model.add(Dense(3,activation='softmax'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i4uiTH8plmNX","colab_type":"text"},"source":["## Question 7"]},{"cell_type":"markdown","metadata":{"id":"yJL8n8vcSyYz","colab_type":"text"},"source":["### Compile the model\n","- Use SGD as Optimizer\n","- Use categorical_crossentropy as loss function\n","- Use accuracy as metrics"]},{"cell_type":"code","metadata":{"id":"Tc_-fjIEk1ve","colab_type":"code","outputId":"81487970-1e57-40dc-9ff5-a8dee853f011","executionInfo":{"status":"ok","timestamp":1580285773118,"user_tz":-330,"elapsed":868,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["model.compile(loss='categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sihIGbRll_jT"},"source":["## Question 8"]},{"cell_type":"markdown","metadata":{"id":"54ZZCfNGlu0i","colab_type":"text"},"source":["### Summarize the model\n","- Check model layers\n","- Understand number of trainable parameters"]},{"cell_type":"code","metadata":{"id":"elER3F_4ln8n","colab_type":"code","outputId":"e7fb055f-0477-4f44-ddf5-77ef72d0a06b","executionInfo":{"status":"ok","timestamp":1580285779626,"user_tz":-330,"elapsed":859,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["for layer in model.layers:\n","    print(layer.output_shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(None, 12)\n","(None, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eEI8Vsk-X3Pl","colab_type":"code","outputId":"c8c95548-432e-4fc4-e1ad-6fe668a4d4ae","executionInfo":{"status":"ok","timestamp":1580285818563,"user_tz":-330,"elapsed":7147,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["print(model.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_1 (Dense)              (None, 12)                60        \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 39        \n","=================================================================\n","Total params: 99\n","Trainable params: 99\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2PiP7j3Vmj4p"},"source":["## Question 9"]},{"cell_type":"markdown","metadata":{"id":"rWdbfFCXmCHt","colab_type":"text"},"source":["### Fit the model\n","- Give train data as training features and labels\n","- Epochs: 100\n","- Give validation data as testing features and labels"]},{"cell_type":"code","metadata":{"id":"cO1c-5tjmBVZ","colab_type":"code","outputId":"2913c455-6329-40b9-e00f-2538c354df2d","executionInfo":{"status":"ok","timestamp":1580285936592,"user_tz":-330,"elapsed":12563,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["fit_model = model.fit(X_train,y_train,epochs=100,batch_size=1,verbose=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/100\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","112/112 [==============================] - 1s 6ms/step - loss: 1.0116 - acc: 0.5446\n","Epoch 2/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.9571 - acc: 0.5714\n","Epoch 3/100\n","112/112 [==============================] - 0s 978us/step - loss: 0.9022 - acc: 0.7589\n","Epoch 4/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.8467 - acc: 0.7143\n","Epoch 5/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.7978 - acc: 0.6786\n","Epoch 6/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.7548 - acc: 0.6786\n","Epoch 7/100\n","112/112 [==============================] - 0s 986us/step - loss: 0.7187 - acc: 0.6964\n","Epoch 8/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.6887 - acc: 0.7679\n","Epoch 9/100\n","112/112 [==============================] - 0s 971us/step - loss: 0.6630 - acc: 0.7589\n","Epoch 10/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.6422 - acc: 0.7768\n","Epoch 11/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.6247 - acc: 0.7411\n","Epoch 12/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.6092 - acc: 0.7857\n","Epoch 13/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.5952 - acc: 0.8482\n","Epoch 14/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.5845 - acc: 0.7857\n","Epoch 15/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.5744 - acc: 0.7321\n","Epoch 16/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.5650 - acc: 0.8214\n","Epoch 17/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.5568 - acc: 0.7857\n","Epoch 18/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.5493 - acc: 0.9107\n","Epoch 19/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.5415 - acc: 0.8304\n","Epoch 20/100\n","112/112 [==============================] - 0s 995us/step - loss: 0.5357 - acc: 0.8929\n","Epoch 21/100\n","112/112 [==============================] - 0s 986us/step - loss: 0.5288 - acc: 0.9196\n","Epoch 22/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.5239 - acc: 0.8482\n","Epoch 23/100\n","112/112 [==============================] - 0s 908us/step - loss: 0.5183 - acc: 0.8482\n","Epoch 24/100\n","112/112 [==============================] - 0s 905us/step - loss: 0.5128 - acc: 0.8750\n","Epoch 25/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.5075 - acc: 0.8839\n","Epoch 26/100\n","112/112 [==============================] - 0s 960us/step - loss: 0.5022 - acc: 0.9018\n","Epoch 27/100\n","112/112 [==============================] - 0s 965us/step - loss: 0.4983 - acc: 0.8661\n","Epoch 28/100\n","112/112 [==============================] - 0s 989us/step - loss: 0.4909 - acc: 0.8929\n","Epoch 29/100\n","112/112 [==============================] - 0s 982us/step - loss: 0.4892 - acc: 0.8661\n","Epoch 30/100\n","112/112 [==============================] - 0s 928us/step - loss: 0.4841 - acc: 0.9018\n","Epoch 31/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.4797 - acc: 0.8929\n","Epoch 32/100\n","112/112 [==============================] - 0s 921us/step - loss: 0.4747 - acc: 0.9196\n","Epoch 33/100\n","112/112 [==============================] - 0s 922us/step - loss: 0.4702 - acc: 0.9018\n","Epoch 34/100\n","112/112 [==============================] - 0s 966us/step - loss: 0.4660 - acc: 0.9196\n","Epoch 35/100\n","112/112 [==============================] - 0s 983us/step - loss: 0.4631 - acc: 0.8929\n","Epoch 36/100\n","112/112 [==============================] - 0s 947us/step - loss: 0.4561 - acc: 0.9375\n","Epoch 37/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.4520 - acc: 0.9018\n","Epoch 38/100\n","112/112 [==============================] - 0s 968us/step - loss: 0.4486 - acc: 0.9286\n","Epoch 39/100\n","112/112 [==============================] - 0s 908us/step - loss: 0.4414 - acc: 0.9107\n","Epoch 40/100\n","112/112 [==============================] - 0s 995us/step - loss: 0.4401 - acc: 0.9107\n","Epoch 41/100\n","112/112 [==============================] - 0s 983us/step - loss: 0.4351 - acc: 0.9286\n","Epoch 42/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.4375 - acc: 0.8661\n","Epoch 43/100\n","112/112 [==============================] - 0s 890us/step - loss: 0.4321 - acc: 0.9107\n","Epoch 44/100\n","112/112 [==============================] - 0s 916us/step - loss: 0.4272 - acc: 0.9286\n","Epoch 45/100\n","112/112 [==============================] - 0s 913us/step - loss: 0.4263 - acc: 0.9196\n","Epoch 46/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.4186 - acc: 0.9286\n","Epoch 47/100\n","112/112 [==============================] - 0s 894us/step - loss: 0.4172 - acc: 0.9107\n","Epoch 48/100\n","112/112 [==============================] - 0s 981us/step - loss: 0.4107 - acc: 0.9196\n","Epoch 49/100\n","112/112 [==============================] - 0s 879us/step - loss: 0.4093 - acc: 0.9107\n","Epoch 50/100\n","112/112 [==============================] - 0s 964us/step - loss: 0.4044 - acc: 0.9107\n","Epoch 51/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.4013 - acc: 0.9286\n","Epoch 52/100\n","112/112 [==============================] - 0s 952us/step - loss: 0.4007 - acc: 0.9196\n","Epoch 53/100\n","112/112 [==============================] - 0s 961us/step - loss: 0.3938 - acc: 0.9375\n","Epoch 54/100\n","112/112 [==============================] - 0s 884us/step - loss: 0.3825 - acc: 0.9196\n","Epoch 55/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3941 - acc: 0.8929\n","Epoch 56/100\n","112/112 [==============================] - 0s 896us/step - loss: 0.3896 - acc: 0.8929\n","Epoch 57/100\n","112/112 [==============================] - 0s 901us/step - loss: 0.3934 - acc: 0.9107\n","Epoch 58/100\n","112/112 [==============================] - 0s 973us/step - loss: 0.3852 - acc: 0.9286\n","Epoch 59/100\n","112/112 [==============================] - 0s 868us/step - loss: 0.3865 - acc: 0.9107\n","Epoch 60/100\n","112/112 [==============================] - 0s 884us/step - loss: 0.3688 - acc: 0.9375\n","Epoch 61/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3784 - acc: 0.9286\n","Epoch 62/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3780 - acc: 0.9464\n","Epoch 63/100\n","112/112 [==============================] - 0s 878us/step - loss: 0.3707 - acc: 0.9375\n","Epoch 64/100\n","112/112 [==============================] - 0s 900us/step - loss: 0.3715 - acc: 0.9286\n","Epoch 65/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3660 - acc: 0.9375\n","Epoch 66/100\n","112/112 [==============================] - 0s 983us/step - loss: 0.3684 - acc: 0.9375\n","Epoch 67/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3668 - acc: 0.9196\n","Epoch 68/100\n","112/112 [==============================] - 0s 972us/step - loss: 0.3658 - acc: 0.9107\n","Epoch 69/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3685 - acc: 0.9107\n","Epoch 70/100\n","112/112 [==============================] - 0s 914us/step - loss: 0.3541 - acc: 0.9464\n","Epoch 71/100\n","112/112 [==============================] - 0s 947us/step - loss: 0.3559 - acc: 0.9286\n","Epoch 72/100\n","112/112 [==============================] - 0s 987us/step - loss: 0.3617 - acc: 0.9107\n","Epoch 73/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3556 - acc: 0.9286\n","Epoch 74/100\n","112/112 [==============================] - 0s 902us/step - loss: 0.3519 - acc: 0.9286\n","Epoch 75/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3455 - acc: 0.9286\n","Epoch 76/100\n","112/112 [==============================] - 0s 922us/step - loss: 0.3495 - acc: 0.9286\n","Epoch 77/100\n","112/112 [==============================] - 0s 910us/step - loss: 0.3424 - acc: 0.9375\n","Epoch 78/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3424 - acc: 0.9375\n","Epoch 79/100\n","112/112 [==============================] - 0s 879us/step - loss: 0.3449 - acc: 0.9018\n","Epoch 80/100\n","112/112 [==============================] - 0s 903us/step - loss: 0.3382 - acc: 0.9196\n","Epoch 81/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3485 - acc: 0.9196\n","Epoch 82/100\n","112/112 [==============================] - 0s 890us/step - loss: 0.3325 - acc: 0.9286\n","Epoch 83/100\n","112/112 [==============================] - 0s 991us/step - loss: 0.3353 - acc: 0.9286\n","Epoch 84/100\n","112/112 [==============================] - 0s 854us/step - loss: 0.3257 - acc: 0.9464\n","Epoch 85/100\n","112/112 [==============================] - 0s 917us/step - loss: 0.3330 - acc: 0.9107\n","Epoch 86/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3426 - acc: 0.9018\n","Epoch 87/100\n","112/112 [==============================] - 0s 907us/step - loss: 0.3236 - acc: 0.9196\n","Epoch 88/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3331 - acc: 0.9196\n","Epoch 89/100\n","112/112 [==============================] - 0s 976us/step - loss: 0.3305 - acc: 0.9286\n","Epoch 90/100\n","112/112 [==============================] - 0s 960us/step - loss: 0.3262 - acc: 0.9375\n","Epoch 91/100\n","112/112 [==============================] - 0s 991us/step - loss: 0.3178 - acc: 0.9464\n","Epoch 92/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3191 - acc: 0.9375\n","Epoch 93/100\n","112/112 [==============================] - 0s 991us/step - loss: 0.3253 - acc: 0.9375\n","Epoch 94/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3200 - acc: 0.9196\n","Epoch 95/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3277 - acc: 0.9286\n","Epoch 96/100\n","112/112 [==============================] - 0s 965us/step - loss: 0.3212 - acc: 0.9107\n","Epoch 97/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3174 - acc: 0.9375\n","Epoch 98/100\n","112/112 [==============================] - 0s 908us/step - loss: 0.3157 - acc: 0.9464\n","Epoch 99/100\n","112/112 [==============================] - 0s 999us/step - loss: 0.3144 - acc: 0.9464\n","Epoch 100/100\n","112/112 [==============================] - 0s 1ms/step - loss: 0.3103 - acc: 0.9107\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"re9ItAR3yS3J","colab_type":"text"},"source":["## Question 10"]},{"cell_type":"markdown","metadata":{"id":"liw0IFf9yVqH","colab_type":"text"},"source":["### Make predictions\n","- Predict labels on one row"]},{"cell_type":"code","metadata":{"id":"H5sBybi6mlLl","colab_type":"code","outputId":"4ab87e2d-5493-4623-e2a3-09d652095388","executionInfo":{"status":"ok","timestamp":1580285940536,"user_tz":-330,"elapsed":797,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["Predict = np.round(model.predict(X_test))\n","Predict[0:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 1., 0.]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"hSUgMq3m0bG7","colab_type":"text"},"source":["### Compare the prediction with actual label\n","- Print the same row as done in the previous step but of actual labels"]},{"cell_type":"code","metadata":{"id":"fd1zOIxtgMrE","colab_type":"code","outputId":"8b9de922-4aca-45f6-988f-c4a1ae1cb148","executionInfo":{"status":"ok","timestamp":1580285967267,"user_tz":-330,"elapsed":811,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["score =model.evaluate(X_test,y_test,verbose=1)\n","print(score)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["38/38 [==============================] - 0s 156us/step\n","[0.26818402111530304, 0.9736842105263158]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FrTKwbgE7NFT","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a1UBYPNp5Tn1","colab_type":"text"},"source":["# Stock prices dataset\n","The data is of tock exchange's stock listings for each trading day of 2010 to 2016.\n","\n","## Description\n","A brief description of columns.\n","- open: The opening market price of the equity symbol on the date\n","- high: The highest market price of the equity symbol on the date\n","- low: The lowest recorded market price of the equity symbol on the date\n","- close: The closing recorded price of the equity symbol on the date\n","- symbol: Symbol of the listed company\n","- volume: Total traded volume of the equity symbol on the date\n","- date: Date of record"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ctH_ZW5g-M3g"},"source":["### Specifying the TensorFlow version\n","Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vQbdODpH-M3r","outputId":"bff339c5-d233-45a7-8c7b-23b71a887337","executionInfo":{"status":"ok","timestamp":1580292368367,"user_tz":-330,"elapsed":785,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 2.x"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nFQWH1tj-M38"},"source":["### Import TensorFlow\n","Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ho5n-xhd-M3_","outputId":"a63c1452-5605-49ab-f658-afbcd95adc75","executionInfo":{"status":"ok","timestamp":1580292379041,"user_tz":-330,"elapsed":8752,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["2.1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tgkl0qu6-M4F"},"source":["### Set random seed"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TKgTyuA3-M4G","colab":{}},"source":["tf.random.set_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_88voqAH-O6J","colab_type":"text"},"source":["## Question 1"]},{"cell_type":"markdown","metadata":{"id":"dRHCeJqP-evf","colab_type":"text"},"source":["### Load the data\n","- load the csv file and read it using pandas\n","- file name is prices.csv"]},{"cell_type":"code","metadata":{"id":"cKVH5v7r-RmC","colab_type":"code","outputId":"4c65852f-f842-40ce-afca-7bb269e54b54","executionInfo":{"status":"ok","timestamp":1580293118877,"user_tz":-330,"elapsed":721362,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":74}},"source":["# run this cell to upload file if you are using google colab\n","from google.colab import files\n","uploaded=files.upload()"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-6e46a05f-cf97-4c94-aa49-d99c6fefed45\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-6e46a05f-cf97-4c94-aa49-d99c6fefed45\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving prices.csv to prices (1).csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-gDC6cSW_FSK","colab_type":"code","outputId":"99da85ca-f0dc-475e-832f-7579a33e9375","executionInfo":{"status":"ok","timestamp":1580293196591,"user_tz":-330,"elapsed":1983,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["import numpy as np\n","import pandas as pd\n","import io\n","\n","prices_df = pd.read_csv(io.StringIO(uploaded['prices.csv'].decode('utf-8')))\n","prices_df.head(5)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>symbol</th>\n","      <th>open</th>\n","      <th>close</th>\n","      <th>low</th>\n","      <th>high</th>\n","      <th>volume</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2016-01-05 00:00:00</td>\n","      <td>WLTW</td>\n","      <td>123.430000</td>\n","      <td>125.839996</td>\n","      <td>122.309998</td>\n","      <td>126.250000</td>\n","      <td>2163600.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2016-01-06 00:00:00</td>\n","      <td>WLTW</td>\n","      <td>125.239998</td>\n","      <td>119.980003</td>\n","      <td>119.940002</td>\n","      <td>125.540001</td>\n","      <td>2386400.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2016-01-07 00:00:00</td>\n","      <td>WLTW</td>\n","      <td>116.379997</td>\n","      <td>114.949997</td>\n","      <td>114.930000</td>\n","      <td>119.739998</td>\n","      <td>2489500.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2016-01-08 00:00:00</td>\n","      <td>WLTW</td>\n","      <td>115.480003</td>\n","      <td>116.620003</td>\n","      <td>113.500000</td>\n","      <td>117.440002</td>\n","      <td>2006300.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2016-01-11 00:00:00</td>\n","      <td>WLTW</td>\n","      <td>117.010002</td>\n","      <td>114.970001</td>\n","      <td>114.089996</td>\n","      <td>117.330002</td>\n","      <td>1408600.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  date symbol        open  ...         low        high     volume\n","0  2016-01-05 00:00:00   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n","1  2016-01-06 00:00:00   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n","2  2016-01-07 00:00:00   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n","3  2016-01-08 00:00:00   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n","4  2016-01-11 00:00:00   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"HlLKVPVH_BCT","colab_type":"text"},"source":["## Question 2"]},{"cell_type":"markdown","metadata":{"id":"9J4BlzVA_gZd","colab_type":"text"},"source":["### Drop columnns\n","- drop \"date\" and \"symbol\" column from the data"]},{"cell_type":"code","metadata":{"id":"IKEK8aEE_Csx","colab_type":"code","colab":{}},"source":["prices_df.drop(['date','symbol'],axis=1,inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VeQ49Yg-n_5W","colab_type":"code","outputId":"2cc22140-b845-464a-9869-35896a3c6edb","executionInfo":{"status":"ok","timestamp":1580293205277,"user_tz":-330,"elapsed":1568,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["prices_df.head(5)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>open</th>\n","      <th>close</th>\n","      <th>low</th>\n","      <th>high</th>\n","      <th>volume</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>123.430000</td>\n","      <td>125.839996</td>\n","      <td>122.309998</td>\n","      <td>126.250000</td>\n","      <td>2163600.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>125.239998</td>\n","      <td>119.980003</td>\n","      <td>119.940002</td>\n","      <td>125.540001</td>\n","      <td>2386400.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>116.379997</td>\n","      <td>114.949997</td>\n","      <td>114.930000</td>\n","      <td>119.739998</td>\n","      <td>2489500.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>115.480003</td>\n","      <td>116.620003</td>\n","      <td>113.500000</td>\n","      <td>117.440002</td>\n","      <td>2006300.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>117.010002</td>\n","      <td>114.970001</td>\n","      <td>114.089996</td>\n","      <td>117.330002</td>\n","      <td>1408600.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         open       close         low        high     volume\n","0  123.430000  125.839996  122.309998  126.250000  2163600.0\n","1  125.239998  119.980003  119.940002  125.540001  2386400.0\n","2  116.379997  114.949997  114.930000  119.739998  2489500.0\n","3  115.480003  116.620003  113.500000  117.440002  2006300.0\n","4  117.010002  114.970001  114.089996  117.330002  1408600.0"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"cTPhO6v-AiZt","colab_type":"text"},"source":["## Question 3"]},{"cell_type":"markdown","metadata":{"id":"SsZXmF3NAkna","colab_type":"text"},"source":["### Take initial rows\n","- Take first 1000 rows from the data\n","- This step is done to make the execution faster"]},{"cell_type":"code","metadata":{"id":"aKs04iIHAjxN","colab_type":"code","colab":{}},"source":["prices_data = prices_df.iloc[0:1000,:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8qc579mofI5","colab_type":"code","outputId":"429c74c6-e416-495e-e5da-862a6dac626a","executionInfo":{"status":"ok","timestamp":1580293214505,"user_tz":-330,"elapsed":790,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["prices_data.shape"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 5)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"6vGtnapgBIJm","colab_type":"text"},"source":["## Question 4"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C8u_jlbABTip"},"source":["### Get features and label from the dataset in separate variable\n","- Take \"open\", \"close\", \"low\", \"high\" columns as features\n","- Take \"volume\" column as label\n","- Normalize label column by dividing it with 1000000"]},{"cell_type":"code","metadata":{"id":"xQjCMzUXBJbg","colab_type":"code","colab":{}},"source":["X = prices_data.drop(['volume'],axis=1)\n","Y = prices_data['volume']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Mv4ocI1pX13","colab_type":"code","colab":{}},"source":["Y = Y/1000000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"leQUMh-5plNg","colab_type":"code","outputId":"c31cc8d2-c8ea-427b-f4d6-d06b33c219f2","executionInfo":{"status":"ok","timestamp":1580293227180,"user_tz":-330,"elapsed":801,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["Y.head(5)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    2.1636\n","1    2.3864\n","2    2.4895\n","3    2.0063\n","4    1.4086\n","Name: volume, dtype: float64"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"aTAKzlxZBz0z","colab_type":"text"},"source":["## Question 5"]},{"cell_type":"markdown","metadata":{"id":"IfY8Km1Zzyt2","colab_type":"text"},"source":["### Convert data\n","- Convert features and labels to numpy array\n","- Convert their data type to \"float32\""]},{"cell_type":"code","metadata":{"id":"-UYCD5RWrNFz","colab_type":"code","colab":{}},"source":["X= np.array(X)\n","Y=np.array(Y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qSP2xpUJqbjW","colab_type":"code","colab":{}},"source":["#X=X.astype('float32')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGSwkWz-rQgI","colab_type":"code","colab":{}},"source":["#y=y.astype('float32')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOFAlRhUrmPS","colab_type":"code","outputId":"a56e0fbd-bead-4792-d611-4bc46a619b26","executionInfo":{"status":"ok","timestamp":1580293240436,"user_tz":-330,"elapsed":898,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["print(\"X[0]: \",X[0])\n","print(\"Y[0]: \",Y[0])\n","print(\"Len of X:{} and length of Y:{}\".format(len(X), len(Y)))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["X[0]:  [123.43     125.839996 122.309998 126.25    ]\n","Y[0]:  2.1636\n","Len of X:1000 and length of Y:1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3TWpN0nVTpUx"},"source":["## Question 6"]},{"cell_type":"markdown","metadata":{"id":"WQ1FKEs-4btX","colab_type":"text"},"source":["### Normalize data\n","- Normalize features\n","- Use tf.math.l2_normalize to normalize features\n","- You can read more about it here https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize"]},{"cell_type":"code","metadata":{"id":"V0Tfe00X78wB","colab_type":"code","outputId":"dc902392-3325-4e0f-d6b0-4aac5e18ef1c","executionInfo":{"status":"ok","timestamp":1580293248148,"user_tz":-330,"elapsed":781,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["X_normalized = tf.math.l2_normalize(X,axis=0)\n","X_normalized"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1000, 4), dtype=float64, numpy=\n","array([[0.04405065, 0.0448811 , 0.04406771, 0.04469663],\n","       [0.04469661, 0.04279112, 0.04321381, 0.04444527],\n","       [0.04153459, 0.04099716, 0.04140873, 0.04239187],\n","       ...,\n","       [0.01010706, 0.01026088, 0.01009187, 0.01019968],\n","       [0.01570306, 0.01597802, 0.01576292, 0.01586421],\n","       [0.01287651, 0.01324606, 0.01297423, 0.01318064]])>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wmXUGc2oTspa"},"source":["## Question 7"]},{"cell_type":"markdown","metadata":{"id":"VJelDMpzxs0L","colab_type":"text"},"source":["### Define weight and bias\n","- Initialize weight and bias with tf.zeros\n","- tf.zeros is an initializer that generates tensors initialized to 0\n","- Specify the value for shape"]},{"cell_type":"code","metadata":{"id":"8o9RPWVTxs0O","colab_type":"code","colab":{}},"source":["w = tf.Variable(tf.zeros(shape=(4,1)))\n","b = tf.Variable(tf.zeros(shape=(1)))\n","w = tf.dtypes.cast(w, tf.float64)\n","b = tf.dtypes.cast(b, tf.float64)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjpoFr_luRLW","colab_type":"code","outputId":"c7f2cf74-e798-45cb-9c89-e4afac48f4cb","executionInfo":{"status":"ok","timestamp":1580293269697,"user_tz":-330,"elapsed":812,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["print(\"weight: \",w)\n","print(\"bias: \",b)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["weight:  tf.Tensor(\n","[[0.]\n"," [0.]\n"," [0.]\n"," [0.]], shape=(4, 1), dtype=float64)\n","bias:  tf.Tensor([0.], shape=(1,), dtype=float64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8a0wr94aTyjg"},"source":["## Question 8"]},{"cell_type":"markdown","metadata":{"id":"zMXXYdOSxs0Q","colab_type":"text"},"source":["### Get prediction\n","- Define a function to get prediction\n","- Approach: prediction = (X * W) + b; here is X is features"]},{"cell_type":"code","metadata":{"id":"U8Cty1y0xs0S","colab_type":"code","colab":{}},"source":["@tf.function\n","def prediction(X,w,b):\n","    yhat = tf.add(tf.matmul(X,w),b)\n","    return yhat\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQmS3Tauxs0V","colab_type":"text"},"source":["### Calculate loss\n","- Calculate loss using predictions\n","- Define a function to calculate loss\n","- We are calculating mean squared error"]},{"cell_type":"code","metadata":{"id":"-FRXmDd5xs0X","colab_type":"code","colab":{}},"source":["@tf.function\n","def loss_function(Y_actual, Y_predicted):\n","  error = Y_actual - Y_predicted\n","  loss = tf.reduce_mean(tf.square(error))\n","  return loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZbBpnOtfT0wd"},"source":["## Question 9"]},{"cell_type":"markdown","metadata":{"id":"bkOzAUUsTmF_","colab_type":"text"},"source":["### Define a function to train the model\n","1.   Record all the mathematical steps to calculate Loss\n","2.   Calculate Gradients of Loss w.r.t weights and bias\n","3.   Update Weights and Bias based on gradients and learning rate to minimize loss"]},{"cell_type":"code","metadata":{"id":"2R4uieGYLYtM","colab_type":"code","colab":{}},"source":["def train(X,Y, w,b,learning_rate = 0.1):\n","  \n","  with tf.GradientTape() as tape:\n","    tape.watch([w,b])\n","    Yhat = prediction(X, w,b)\n","    loss = loss_function(Y, Yhat)\n","  \n","  #evalute the gradient with the respect to the paramters\n","  dW, db = tape.gradient(loss, [w, b])\n","  \n","  #update the paramters using Gradient Descent  \n","  w = w - learning_rate * dW\n","  b = b - learning_rate * db\n","\n","  return w, b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FbdkbFcEisup","colab_type":"code","colab":{}},"source":["#X= np.array([prices_features])\n","#y= np.array([prices_label])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AW4SEP8kT2ls"},"source":["## Question 10"]},{"cell_type":"markdown","metadata":{"id":"yeN0deOvT81N","colab_type":"text"},"source":["### Train the model for 100 epochs \n","- Observe the training loss at every iteration"]},{"cell_type":"code","metadata":{"id":"Jjkn4gUgLevE","colab_type":"code","outputId":"ee1671e4-72ba-4f8e-a9be-800701feb587","executionInfo":{"status":"ok","timestamp":1580293384887,"user_tz":-330,"elapsed":20520,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["epochs = 1001\n","for i in range(epochs):\n","  w, b = train(X_normalized,Y, w,b)\n","  print(\"Training Loss on iteration \", i, loss_function(Y, prediction(X_normalized, w,b)).numpy())\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Training Loss on iteration  0 209.04581679745903\n","Training Loss on iteration  1 209.04581675915944\n","Training Loss on iteration  2 209.04581672088628\n","Training Loss on iteration  3 209.0458166826397\n","Training Loss on iteration  4 209.0458166444195\n","Training Loss on iteration  5 209.04581660622574\n","Training Loss on iteration  6 209.04581656805843\n","Training Loss on iteration  7 209.0458165299175\n","Training Loss on iteration  8 209.04581649180292\n","Training Loss on iteration  9 209.0458164537148\n","Training Loss on iteration  10 209.04581641565292\n","Training Loss on iteration  11 209.04581637761746\n","Training Loss on iteration  12 209.04581633960825\n","Training Loss on iteration  13 209.04581630162536\n","Training Loss on iteration  14 209.04581626366874\n","Training Loss on iteration  15 209.04581622573838\n","Training Loss on iteration  16 209.04581618783428\n","Training Loss on iteration  17 209.04581614995635\n","Training Loss on iteration  18 209.04581611210466\n","Training Loss on iteration  19 209.04581607427912\n","Training Loss on iteration  20 209.04581603647978\n","Training Loss on iteration  21 209.0458159987066\n","Training Loss on iteration  22 209.0458159609595\n","Training Loss on iteration  23 209.0458159232385\n","Training Loss on iteration  24 209.04581588554365\n","Training Loss on iteration  25 209.04581584787488\n","Training Loss on iteration  26 209.0458158102321\n","Training Loss on iteration  27 209.04581577261544\n","Training Loss on iteration  28 209.04581573502475\n","Training Loss on iteration  29 209.04581569746009\n","Training Loss on iteration  30 209.04581565992137\n","Training Loss on iteration  31 209.0458156224087\n","Training Loss on iteration  32 209.04581558492188\n","Training Loss on iteration  33 209.04581554746105\n","Training Loss on iteration  34 209.0458155100261\n","Training Loss on iteration  35 209.04581547261708\n","Training Loss on iteration  36 209.04581543523395\n","Training Loss on iteration  37 209.04581539787665\n","Training Loss on iteration  38 209.0458153605452\n","Training Loss on iteration  39 209.04581532323957\n","Training Loss on iteration  40 209.04581528595972\n","Training Loss on iteration  41 209.04581524870568\n","Training Loss on iteration  42 209.04581521147742\n","Training Loss on iteration  43 209.04581517427494\n","Training Loss on iteration  44 209.04581513709817\n","Training Loss on iteration  45 209.0458150999471\n","Training Loss on iteration  46 209.04581506282176\n","Training Loss on iteration  47 209.04581502572205\n","Training Loss on iteration  48 209.04581498864806\n","Training Loss on iteration  49 209.04581495159965\n","Training Loss on iteration  50 209.0458149145769\n","Training Loss on iteration  51 209.0458148775798\n","Training Loss on iteration  52 209.04581484060827\n","Training Loss on iteration  53 209.0458148036623\n","Training Loss on iteration  54 209.0458147667419\n","Training Loss on iteration  55 209.04581472984702\n","Training Loss on iteration  56 209.04581469297767\n","Training Loss on iteration  57 209.04581465613384\n","Training Loss on iteration  58 209.0458146193155\n","Training Loss on iteration  59 209.04581458252264\n","Training Loss on iteration  60 209.04581454575515\n","Training Loss on iteration  61 209.04581450901318\n","Training Loss on iteration  62 209.0458144722966\n","Training Loss on iteration  63 209.0458144356054\n","Training Loss on iteration  64 209.0458143989396\n","Training Loss on iteration  65 209.04581436229918\n","Training Loss on iteration  66 209.04581432568406\n","Training Loss on iteration  67 209.04581428909432\n","Training Loss on iteration  68 209.04581425252985\n","Training Loss on iteration  69 209.04581421599073\n","Training Loss on iteration  70 209.04581417947682\n","Training Loss on iteration  71 209.0458141429882\n","Training Loss on iteration  72 209.04581410652483\n","Training Loss on iteration  73 209.04581407008666\n","Training Loss on iteration  74 209.0458140336737\n","Training Loss on iteration  75 209.0458139972859\n","Training Loss on iteration  76 209.0458139609233\n","Training Loss on iteration  77 209.04581392458587\n","Training Loss on iteration  78 209.04581388827359\n","Training Loss on iteration  79 209.0458138519864\n","Training Loss on iteration  80 209.0458138157243\n","Training Loss on iteration  81 209.0458137794873\n","Training Loss on iteration  82 209.04581374327537\n","Training Loss on iteration  83 209.0458137070885\n","Training Loss on iteration  84 209.04581367092663\n","Training Loss on iteration  85 209.0458136347898\n","Training Loss on iteration  86 209.04581359867797\n","Training Loss on iteration  87 209.0458135625911\n","Training Loss on iteration  88 209.04581352652923\n","Training Loss on iteration  89 209.04581349049224\n","Training Loss on iteration  90 209.04581345448022\n","Training Loss on iteration  91 209.0458134184931\n","Training Loss on iteration  92 209.0458133825309\n","Training Loss on iteration  93 209.04581334659355\n","Training Loss on iteration  94 209.04581331068104\n","Training Loss on iteration  95 209.04581327479346\n","Training Loss on iteration  96 209.04581323893063\n","Training Loss on iteration  97 209.04581320309262\n","Training Loss on iteration  98 209.0458131672794\n","Training Loss on iteration  99 209.04581313149097\n","Training Loss on iteration  100 209.04581309572728\n","Training Loss on iteration  101 209.04581305998832\n","Training Loss on iteration  102 209.0458130242741\n","Training Loss on iteration  103 209.0458129885846\n","Training Loss on iteration  104 209.04581295291976\n","Training Loss on iteration  105 209.04581291727962\n","Training Loss on iteration  106 209.0458128816641\n","Training Loss on iteration  107 209.04581284607326\n","Training Loss on iteration  108 209.045812810507\n","Training Loss on iteration  109 209.04581277496538\n","Training Loss on iteration  110 209.0458127394483\n","Training Loss on iteration  111 209.04581270395582\n","Training Loss on iteration  112 209.04581266848788\n","Training Loss on iteration  113 209.04581263304448\n","Training Loss on iteration  114 209.04581259762563\n","Training Loss on iteration  115 209.04581256223122\n","Training Loss on iteration  116 209.04581252686134\n","Training Loss on iteration  117 209.04581249151587\n","Training Loss on iteration  118 209.04581245619494\n","Training Loss on iteration  119 209.0458124208984\n","Training Loss on iteration  120 209.04581238562622\n","Training Loss on iteration  121 209.04581235037853\n","Training Loss on iteration  122 209.04581231515516\n","Training Loss on iteration  123 209.04581227995615\n","Training Loss on iteration  124 209.04581224478156\n","Training Loss on iteration  125 209.04581220963124\n","Training Loss on iteration  126 209.04581217450524\n","Training Loss on iteration  127 209.04581213940355\n","Training Loss on iteration  128 209.04581210432613\n","Training Loss on iteration  129 209.04581206927298\n","Training Loss on iteration  130 209.0458120342441\n","Training Loss on iteration  131 209.04581199923945\n","Training Loss on iteration  132 209.04581196425897\n","Training Loss on iteration  133 209.04581192930272\n","Training Loss on iteration  134 209.04581189437067\n","Training Loss on iteration  135 209.04581185946273\n","Training Loss on iteration  136 209.045811824579\n","Training Loss on iteration  137 209.0458117897194\n","Training Loss on iteration  138 209.04581175488389\n","Training Loss on iteration  139 209.04581172007244\n","Training Loss on iteration  140 209.04581168528512\n","Training Loss on iteration  141 209.04581165052184\n","Training Loss on iteration  142 209.0458116157826\n","Training Loss on iteration  143 209.04581158106743\n","Training Loss on iteration  144 209.04581154637626\n","Training Loss on iteration  145 209.04581151170905\n","Training Loss on iteration  146 209.04581147706585\n","Training Loss on iteration  147 209.04581144244662\n","Training Loss on iteration  148 209.04581140785135\n","Training Loss on iteration  149 209.04581137328\n","Training Loss on iteration  150 209.04581133873253\n","Training Loss on iteration  151 209.045811304209\n","Training Loss on iteration  152 209.04581126970933\n","Training Loss on iteration  153 209.0458112352335\n","Training Loss on iteration  154 209.04581120078157\n","Training Loss on iteration  155 209.04581116635347\n","Training Loss on iteration  156 209.04581113194916\n","Training Loss on iteration  157 209.04581109756865\n","Training Loss on iteration  158 209.04581106321194\n","Training Loss on iteration  159 209.045811028879\n","Training Loss on iteration  160 209.0458109945698\n","Training Loss on iteration  161 209.04581096028429\n","Training Loss on iteration  162 209.04581092602254\n","Training Loss on iteration  163 209.0458108917845\n","Training Loss on iteration  164 209.04581085757013\n","Training Loss on iteration  165 209.04581082337947\n","Training Loss on iteration  166 209.04581078921237\n","Training Loss on iteration  167 209.04581075506897\n","Training Loss on iteration  168 209.04581072094916\n","Training Loss on iteration  169 209.045810686853\n","Training Loss on iteration  170 209.04581065278035\n","Training Loss on iteration  171 209.0458106187313\n","Training Loss on iteration  172 209.04581058470583\n","Training Loss on iteration  173 209.0458105507039\n","Training Loss on iteration  174 209.04581051672545\n","Training Loss on iteration  175 209.04581048277055\n","Training Loss on iteration  176 209.04581044883912\n","Training Loss on iteration  177 209.04581041493114\n","Training Loss on iteration  178 209.04581038104666\n","Training Loss on iteration  179 209.04581034718555\n","Training Loss on iteration  180 209.04581031334794\n","Training Loss on iteration  181 209.0458102795337\n","Training Loss on iteration  182 209.04581024574287\n","Training Loss on iteration  183 209.0458102119754\n","Training Loss on iteration  184 209.04581017823128\n","Training Loss on iteration  185 209.0458101445105\n","Training Loss on iteration  186 209.04581011081305\n","Training Loss on iteration  187 209.0458100771389\n","Training Loss on iteration  188 209.04581004348807\n","Training Loss on iteration  189 209.0458100098605\n","Training Loss on iteration  190 209.04580997625627\n","Training Loss on iteration  191 209.04580994267516\n","Training Loss on iteration  192 209.0458099091173\n","Training Loss on iteration  193 209.04580987558273\n","Training Loss on iteration  194 209.04580984207132\n","Training Loss on iteration  195 209.04580980858307\n","Training Loss on iteration  196 209.04580977511802\n","Training Loss on iteration  197 209.0458097416761\n","Training Loss on iteration  198 209.0458097082573\n","Training Loss on iteration  199 209.04580967486166\n","Training Loss on iteration  200 209.0458096414891\n","Training Loss on iteration  201 209.04580960813962\n","Training Loss on iteration  202 209.04580957481323\n","Training Loss on iteration  203 209.04580954150987\n","Training Loss on iteration  204 209.04580950822952\n","Training Loss on iteration  205 209.04580947497223\n","Training Loss on iteration  206 209.04580944173796\n","Training Loss on iteration  207 209.04580940852665\n","Training Loss on iteration  208 209.0458093753383\n","Training Loss on iteration  209 209.04580934217293\n","Training Loss on iteration  210 209.04580930903055\n","Training Loss on iteration  211 209.04580927591104\n","Training Loss on iteration  212 209.0458092428144\n","Training Loss on iteration  213 209.04580920974072\n","Training Loss on iteration  214 209.04580917668991\n","Training Loss on iteration  215 209.04580914366196\n","Training Loss on iteration  216 209.04580911065685\n","Training Loss on iteration  217 209.04580907767456\n","Training Loss on iteration  218 209.0458090447151\n","Training Loss on iteration  219 209.04580901177843\n","Training Loss on iteration  220 209.04580897886456\n","Training Loss on iteration  221 209.04580894597348\n","Training Loss on iteration  222 209.04580891310513\n","Training Loss on iteration  223 209.04580888025947\n","Training Loss on iteration  224 209.04580884743658\n","Training Loss on iteration  225 209.04580881463642\n","Training Loss on iteration  226 209.0458087818589\n","Training Loss on iteration  227 209.04580874910408\n","Training Loss on iteration  228 209.04580871637188\n","Training Loss on iteration  229 209.04580868366236\n","Training Loss on iteration  230 209.04580865097543\n","Training Loss on iteration  231 209.04580861831116\n","Training Loss on iteration  232 209.04580858566945\n","Training Loss on iteration  233 209.04580855305036\n","Training Loss on iteration  234 209.04580852045382\n","Training Loss on iteration  235 209.04580848787978\n","Training Loss on iteration  236 209.04580845532834\n","Training Loss on iteration  237 209.04580842279938\n","Training Loss on iteration  238 209.04580839029293\n","Training Loss on iteration  239 209.04580835780894\n","Training Loss on iteration  240 209.04580832534748\n","Training Loss on iteration  241 209.04580829290842\n","Training Loss on iteration  242 209.0458082604918\n","Training Loss on iteration  243 209.04580822809763\n","Training Loss on iteration  244 209.0458081957259\n","Training Loss on iteration  245 209.0458081633765\n","Training Loss on iteration  246 209.0458081310495\n","Training Loss on iteration  247 209.04580809874486\n","Training Loss on iteration  248 209.04580806646257\n","Training Loss on iteration  249 209.04580803420262\n","Training Loss on iteration  250 209.045808001965\n","Training Loss on iteration  251 209.0458079697497\n","Training Loss on iteration  252 209.0458079375566\n","Training Loss on iteration  253 209.0458079053858\n","Training Loss on iteration  254 209.0458078732373\n","Training Loss on iteration  255 209.04580784111099\n","Training Loss on iteration  256 209.04580780900693\n","Training Loss on iteration  257 209.04580777692505\n","Training Loss on iteration  258 209.04580774486539\n","Training Loss on iteration  259 209.0458077128279\n","Training Loss on iteration  260 209.04580768081257\n","Training Loss on iteration  261 209.0458076488194\n","Training Loss on iteration  262 209.04580761684832\n","Training Loss on iteration  263 209.04580758489936\n","Training Loss on iteration  264 209.04580755297255\n","Training Loss on iteration  265 209.04580752106781\n","Training Loss on iteration  266 209.04580748918514\n","Training Loss on iteration  267 209.0458074573245\n","Training Loss on iteration  268 209.04580742548592\n","Training Loss on iteration  269 209.04580739366938\n","Training Loss on iteration  270 209.04580736187478\n","Training Loss on iteration  271 209.04580733010224\n","Training Loss on iteration  272 209.04580729835163\n","Training Loss on iteration  273 209.04580726662306\n","Training Loss on iteration  274 209.04580723491637\n","Training Loss on iteration  275 209.04580720323165\n","Training Loss on iteration  276 209.0458071715688\n","Training Loss on iteration  277 209.04580713992792\n","Training Loss on iteration  278 209.04580710830885\n","Training Loss on iteration  279 209.04580707671172\n","Training Loss on iteration  280 209.04580704513643\n","Training Loss on iteration  281 209.04580701358302\n","Training Loss on iteration  282 209.04580698205137\n","Training Loss on iteration  283 209.04580695054156\n","Training Loss on iteration  284 209.0458069190535\n","Training Loss on iteration  285 209.04580688758728\n","Training Loss on iteration  286 209.04580685614278\n","Training Loss on iteration  287 209.04580682472007\n","Training Loss on iteration  288 209.04580679331912\n","Training Loss on iteration  289 209.04580676193987\n","Training Loss on iteration  290 209.04580673058229\n","Training Loss on iteration  291 209.0458066992464\n","Training Loss on iteration  292 209.04580666793225\n","Training Loss on iteration  293 209.04580663663972\n","Training Loss on iteration  294 209.04580660536885\n","Training Loss on iteration  295 209.0458065741196\n","Training Loss on iteration  296 209.04580654289197\n","Training Loss on iteration  297 209.04580651168592\n","Training Loss on iteration  298 209.0458064805015\n","Training Loss on iteration  299 209.04580644933864\n","Training Loss on iteration  300 209.04580641819732\n","Training Loss on iteration  301 209.04580638707753\n","Training Loss on iteration  302 209.0458063559793\n","Training Loss on iteration  303 209.04580632490257\n","Training Loss on iteration  304 209.04580629384733\n","Training Loss on iteration  305 209.04580626281356\n","Training Loss on iteration  306 209.0458062318013\n","Training Loss on iteration  307 209.04580620081043\n","Training Loss on iteration  308 209.0458061698411\n","Training Loss on iteration  309 209.04580613889306\n","Training Loss on iteration  310 209.04580610796654\n","Training Loss on iteration  311 209.04580607706137\n","Training Loss on iteration  312 209.04580604617757\n","Training Loss on iteration  313 209.04580601531518\n","Training Loss on iteration  314 209.04580598447407\n","Training Loss on iteration  315 209.0458059536543\n","Training Loss on iteration  316 209.04580592285592\n","Training Loss on iteration  317 209.0458058920788\n","Training Loss on iteration  318 209.04580586132295\n","Training Loss on iteration  319 209.0458058305884\n","Training Loss on iteration  320 209.04580579987515\n","Training Loss on iteration  321 209.04580576918306\n","Training Loss on iteration  322 209.04580573851223\n","Training Loss on iteration  323 209.04580570786266\n","Training Loss on iteration  324 209.04580567723426\n","Training Loss on iteration  325 209.04580564662703\n","Training Loss on iteration  326 209.045805616041\n","Training Loss on iteration  327 209.04580558547613\n","Training Loss on iteration  328 209.04580555493243\n","Training Loss on iteration  329 209.04580552440981\n","Training Loss on iteration  330 209.0458054939083\n","Training Loss on iteration  331 209.0458054634279\n","Training Loss on iteration  332 209.0458054329686\n","Training Loss on iteration  333 209.04580540253033\n","Training Loss on iteration  334 209.04580537211316\n","Training Loss on iteration  335 209.045805341717\n","Training Loss on iteration  336 209.0458053113419\n","Training Loss on iteration  337 209.0458052809878\n","Training Loss on iteration  338 209.04580525065467\n","Training Loss on iteration  339 209.04580522034252\n","Training Loss on iteration  340 209.0458051900514\n","Training Loss on iteration  341 209.04580515978122\n","Training Loss on iteration  342 209.0458051295319\n","Training Loss on iteration  343 209.0458050993036\n","Training Loss on iteration  344 209.0458050690962\n","Training Loss on iteration  345 209.04580503890963\n","Training Loss on iteration  346 209.045805008744\n","Training Loss on iteration  347 209.0458049785992\n","Training Loss on iteration  348 209.0458049484753\n","Training Loss on iteration  349 209.0458049183722\n","Training Loss on iteration  350 209.04580488828992\n","Training Loss on iteration  351 209.04580485822845\n","Training Loss on iteration  352 209.04580482818778\n","Training Loss on iteration  353 209.0458047981679\n","Training Loss on iteration  354 209.0458047681688\n","Training Loss on iteration  355 209.0458047381904\n","Training Loss on iteration  356 209.0458047082328\n","Training Loss on iteration  357 209.0458046782959\n","Training Loss on iteration  358 209.04580464837971\n","Training Loss on iteration  359 209.0458046184842\n","Training Loss on iteration  360 209.0458045886094\n","Training Loss on iteration  361 209.0458045587552\n","Training Loss on iteration  362 209.04580452892174\n","Training Loss on iteration  363 209.04580449910884\n","Training Loss on iteration  364 209.0458044693166\n","Training Loss on iteration  365 209.04580443954498\n","Training Loss on iteration  366 209.04580440979393\n","Training Loss on iteration  367 209.0458043800635\n","Training Loss on iteration  368 209.0458043503536\n","Training Loss on iteration  369 209.04580432066422\n","Training Loss on iteration  370 209.04580429099542\n","Training Loss on iteration  371 209.04580426134714\n","Training Loss on iteration  372 209.0458042317194\n","Training Loss on iteration  373 209.0458042021121\n","Training Loss on iteration  374 209.0458041725253\n","Training Loss on iteration  375 209.04580414295896\n","Training Loss on iteration  376 209.04580411341308\n","Training Loss on iteration  377 209.04580408388765\n","Training Loss on iteration  378 209.04580405438264\n","Training Loss on iteration  379 209.04580402489802\n","Training Loss on iteration  380 209.0458039954338\n","Training Loss on iteration  381 209.04580396599\n","Training Loss on iteration  382 209.04580393656656\n","Training Loss on iteration  383 209.04580390716345\n","Training Loss on iteration  384 209.04580387778068\n","Training Loss on iteration  385 209.04580384841825\n","Training Loss on iteration  386 209.04580381907613\n","Training Loss on iteration  387 209.04580378975427\n","Training Loss on iteration  388 209.04580376045269\n","Training Loss on iteration  389 209.04580373117145\n","Training Loss on iteration  390 209.04580370191044\n","Training Loss on iteration  391 209.04580367266965\n","Training Loss on iteration  392 209.04580364344906\n","Training Loss on iteration  393 209.04580361424874\n","Training Loss on iteration  394 209.04580358506863\n","Training Loss on iteration  395 209.04580355590866\n","Training Loss on iteration  396 209.04580352676885\n","Training Loss on iteration  397 209.04580349764925\n","Training Loss on iteration  398 209.04580346854974\n","Training Loss on iteration  399 209.04580343947038\n","Training Loss on iteration  400 209.04580341041114\n","Training Loss on iteration  401 209.045803381372\n","Training Loss on iteration  402 209.04580335235295\n","Training Loss on iteration  403 209.04580332335397\n","Training Loss on iteration  404 209.04580329437505\n","Training Loss on iteration  405 209.04580326541617\n","Training Loss on iteration  406 209.04580323647735\n","Training Loss on iteration  407 209.04580320755855\n","Training Loss on iteration  408 209.0458031786597\n","Training Loss on iteration  409 209.04580314978088\n","Training Loss on iteration  410 209.04580312092202\n","Training Loss on iteration  411 209.04580309208316\n","Training Loss on iteration  412 209.04580306326423\n","Training Loss on iteration  413 209.04580303446522\n","Training Loss on iteration  414 209.0458030056861\n","Training Loss on iteration  415 209.04580297692698\n","Training Loss on iteration  416 209.0458029481877\n","Training Loss on iteration  417 209.0458029194683\n","Training Loss on iteration  418 209.04580289076875\n","Training Loss on iteration  419 209.04580286208906\n","Training Loss on iteration  420 209.04580283342926\n","Training Loss on iteration  421 209.04580280478925\n","Training Loss on iteration  422 209.04580277616904\n","Training Loss on iteration  423 209.04580274756862\n","Training Loss on iteration  424 209.045802718988\n","Training Loss on iteration  425 209.04580269042717\n","Training Loss on iteration  426 209.04580266188609\n","Training Loss on iteration  427 209.04580263336473\n","Training Loss on iteration  428 209.0458026048631\n","Training Loss on iteration  429 209.04580257638122\n","Training Loss on iteration  430 209.04580254791904\n","Training Loss on iteration  431 209.0458025194765\n","Training Loss on iteration  432 209.04580249105368\n","Training Loss on iteration  433 209.04580246265047\n","Training Loss on iteration  434 209.045802434267\n","Training Loss on iteration  435 209.04580240590306\n","Training Loss on iteration  436 209.04580237755883\n","Training Loss on iteration  437 209.04580234923415\n","Training Loss on iteration  438 209.04580232092906\n","Training Loss on iteration  439 209.04580229264357\n","Training Loss on iteration  440 209.04580226437764\n","Training Loss on iteration  441 209.04580223613132\n","Training Loss on iteration  442 209.04580220790444\n","Training Loss on iteration  443 209.04580217969715\n","Training Loss on iteration  444 209.04580215150935\n","Training Loss on iteration  445 209.04580212334108\n","Training Loss on iteration  446 209.04580209519224\n","Training Loss on iteration  447 209.0458020670629\n","Training Loss on iteration  448 209.04580203895304\n","Training Loss on iteration  449 209.0458020108626\n","Training Loss on iteration  450 209.0458019827916\n","Training Loss on iteration  451 209.04580195474\n","Training Loss on iteration  452 209.04580192670784\n","Training Loss on iteration  453 209.04580189869506\n","Training Loss on iteration  454 209.04580187070164\n","Training Loss on iteration  455 209.0458018427276\n","Training Loss on iteration  456 209.04580181477291\n","Training Loss on iteration  457 209.04580178683753\n","Training Loss on iteration  458 209.0458017589215\n","Training Loss on iteration  459 209.0458017310248\n","Training Loss on iteration  460 209.04580170314736\n","Training Loss on iteration  461 209.04580167528923\n","Training Loss on iteration  462 209.04580164745036\n","Training Loss on iteration  463 209.04580161963074\n","Training Loss on iteration  464 209.04580159183038\n","Training Loss on iteration  465 209.04580156404924\n","Training Loss on iteration  466 209.0458015362873\n","Training Loss on iteration  467 209.0458015085446\n","Training Loss on iteration  468 209.04580148082107\n","Training Loss on iteration  469 209.04580145311672\n","Training Loss on iteration  470 209.04580142543156\n","Training Loss on iteration  471 209.04580139776553\n","Training Loss on iteration  472 209.04580137011862\n","Training Loss on iteration  473 209.04580134249085\n","Training Loss on iteration  474 209.04580131488217\n","Training Loss on iteration  475 209.0458012872926\n","Training Loss on iteration  476 209.04580125972214\n","Training Loss on iteration  477 209.04580123217076\n","Training Loss on iteration  478 209.04580120463842\n","Training Loss on iteration  479 209.04580117712513\n","Training Loss on iteration  480 209.04580114963085\n","Training Loss on iteration  481 209.0458011221556\n","Training Loss on iteration  482 209.04580109469933\n","Training Loss on iteration  483 209.04580106726212\n","Training Loss on iteration  484 209.04580103984387\n","Training Loss on iteration  485 209.0458010124445\n","Training Loss on iteration  486 209.0458009850642\n","Training Loss on iteration  487 209.0458009577028\n","Training Loss on iteration  488 209.04580093036031\n","Training Loss on iteration  489 209.04580090303676\n","Training Loss on iteration  490 209.04580087573206\n","Training Loss on iteration  491 209.0458008484463\n","Training Loss on iteration  492 209.0458008211794\n","Training Loss on iteration  493 209.04580079393133\n","Training Loss on iteration  494 209.04580076670217\n","Training Loss on iteration  495 209.04580073949182\n","Training Loss on iteration  496 209.04580071230026\n","Training Loss on iteration  497 209.04580068512752\n","Training Loss on iteration  498 209.04580065797364\n","Training Loss on iteration  499 209.04580063083844\n","Training Loss on iteration  500 209.04580060372209\n","Training Loss on iteration  501 209.04580057662452\n","Training Loss on iteration  502 209.04580054954562\n","Training Loss on iteration  503 209.0458005224855\n","Training Loss on iteration  504 209.04580049544407\n","Training Loss on iteration  505 209.04580046842136\n","Training Loss on iteration  506 209.04580044141733\n","Training Loss on iteration  507 209.045800414432\n","Training Loss on iteration  508 209.0458003874653\n","Training Loss on iteration  509 209.04580036051732\n","Training Loss on iteration  510 209.04580033358792\n","Training Loss on iteration  511 209.04580030667717\n","Training Loss on iteration  512 209.045800279785\n","Training Loss on iteration  513 209.0458002529115\n","Training Loss on iteration  514 209.04580022605657\n","Training Loss on iteration  515 209.04580019922022\n","Training Loss on iteration  516 209.04580017240238\n","Training Loss on iteration  517 209.04580014560315\n","Training Loss on iteration  518 209.04580011882243\n","Training Loss on iteration  519 209.04580009206023\n","Training Loss on iteration  520 209.04580006531657\n","Training Loss on iteration  521 209.0458000385914\n","Training Loss on iteration  522 209.04580001188472\n","Training Loss on iteration  523 209.0457999851965\n","Training Loss on iteration  524 209.04579995852674\n","Training Loss on iteration  525 209.04579993187548\n","Training Loss on iteration  526 209.04579990524263\n","Training Loss on iteration  527 209.04579987862815\n","Training Loss on iteration  528 209.0457998520321\n","Training Loss on iteration  529 209.04579982545448\n","Training Loss on iteration  530 209.04579979889525\n","Training Loss on iteration  531 209.04579977235434\n","Training Loss on iteration  532 209.04579974583186\n","Training Loss on iteration  533 209.04579971932765\n","Training Loss on iteration  534 209.04579969284183\n","Training Loss on iteration  535 209.04579966637434\n","Training Loss on iteration  536 209.04579963992512\n","Training Loss on iteration  537 209.04579961349418\n","Training Loss on iteration  538 209.04579958708158\n","Training Loss on iteration  539 209.04579956068724\n","Training Loss on iteration  540 209.04579953431113\n","Training Loss on iteration  541 209.04579950795326\n","Training Loss on iteration  542 209.04579948161367\n","Training Loss on iteration  543 209.0457994552922\n","Training Loss on iteration  544 209.04579942898908\n","Training Loss on iteration  545 209.04579940270406\n","Training Loss on iteration  546 209.04579937643726\n","Training Loss on iteration  547 209.04579935018862\n","Training Loss on iteration  548 209.04579932395816\n","Training Loss on iteration  549 209.0457992977458\n","Training Loss on iteration  550 209.04579927155157\n","Training Loss on iteration  551 209.04579924537552\n","Training Loss on iteration  552 209.0457992192175\n","Training Loss on iteration  553 209.0457991930776\n","Training Loss on iteration  554 209.04579916695582\n","Training Loss on iteration  555 209.0457991408521\n","Training Loss on iteration  556 209.04579911476642\n","Training Loss on iteration  557 209.04579908869877\n","Training Loss on iteration  558 209.0457990626492\n","Training Loss on iteration  559 209.0457990366176\n","Training Loss on iteration  560 209.04579901060404\n","Training Loss on iteration  561 209.04579898460847\n","Training Loss on iteration  562 209.04579895863085\n","Training Loss on iteration  563 209.04579893267126\n","Training Loss on iteration  564 209.04579890672957\n","Training Loss on iteration  565 209.04579888080585\n","Training Loss on iteration  566 209.04579885490006\n","Training Loss on iteration  567 209.0457988290122\n","Training Loss on iteration  568 209.04579880314225\n","Training Loss on iteration  569 209.04579877729017\n","Training Loss on iteration  570 209.045798751456\n","Training Loss on iteration  571 209.04579872563963\n","Training Loss on iteration  572 209.04579869984124\n","Training Loss on iteration  573 209.0457986740606\n","Training Loss on iteration  574 209.04579864829785\n","Training Loss on iteration  575 209.04579862255287\n","Training Loss on iteration  576 209.04579859682576\n","Training Loss on iteration  577 209.0457985711164\n","Training Loss on iteration  578 209.04579854542482\n","Training Loss on iteration  579 209.045798519751\n","Training Loss on iteration  580 209.04579849409504\n","Training Loss on iteration  581 209.04579846845675\n","Training Loss on iteration  582 209.04579844283617\n","Training Loss on iteration  583 209.04579841723336\n","Training Loss on iteration  584 209.04579839164822\n","Training Loss on iteration  585 209.04579836608082\n","Training Loss on iteration  586 209.0457983405311\n","Training Loss on iteration  587 209.04579831499905\n","Training Loss on iteration  588 209.04579828948462\n","Training Loss on iteration  589 209.0457982639879\n","Training Loss on iteration  590 209.04579823850878\n","Training Loss on iteration  591 209.04579821304733\n","Training Loss on iteration  592 209.04579818760345\n","Training Loss on iteration  593 209.04579816217716\n","Training Loss on iteration  594 209.0457981367685\n","Training Loss on iteration  595 209.0457981113774\n","Training Loss on iteration  596 209.04579808600386\n","Training Loss on iteration  597 209.04579806064788\n","Training Loss on iteration  598 209.04579803530945\n","Training Loss on iteration  599 209.04579800998852\n","Training Loss on iteration  600 209.04579798468512\n","Training Loss on iteration  601 209.0457979593992\n","Training Loss on iteration  602 209.04579793413077\n","Training Loss on iteration  603 209.04579790887988\n","Training Loss on iteration  604 209.04579788364643\n","Training Loss on iteration  605 209.04579785843038\n","Training Loss on iteration  606 209.04579783323183\n","Training Loss on iteration  607 209.04579780805068\n","Training Loss on iteration  608 209.04579778288698\n","Training Loss on iteration  609 209.04579775774067\n","Training Loss on iteration  610 209.0457977326118\n","Training Loss on iteration  611 209.0457977075002\n","Training Loss on iteration  612 209.0457976824061\n","Training Loss on iteration  613 209.0457976573293\n","Training Loss on iteration  614 209.04579763226982\n","Training Loss on iteration  615 209.04579760722774\n","Training Loss on iteration  616 209.0457975822029\n","Training Loss on iteration  617 209.04579755719544\n","Training Loss on iteration  618 209.04579753220528\n","Training Loss on iteration  619 209.04579750723238\n","Training Loss on iteration  620 209.04579748227673\n","Training Loss on iteration  621 209.04579745733838\n","Training Loss on iteration  622 209.04579743241726\n","Training Loss on iteration  623 209.04579740751345\n","Training Loss on iteration  624 209.04579738262677\n","Training Loss on iteration  625 209.04579735775732\n","Training Loss on iteration  626 209.04579733290512\n","Training Loss on iteration  627 209.04579730807006\n","Training Loss on iteration  628 209.04579728325223\n","Training Loss on iteration  629 209.04579725845159\n","Training Loss on iteration  630 209.04579723366803\n","Training Loss on iteration  631 209.04579720890163\n","Training Loss on iteration  632 209.0457971841524\n","Training Loss on iteration  633 209.04579715942026\n","Training Loss on iteration  634 209.04579713470525\n","Training Loss on iteration  635 209.04579711000733\n","Training Loss on iteration  636 209.04579708532646\n","Training Loss on iteration  637 209.0457970606627\n","Training Loss on iteration  638 209.04579703601598\n","Training Loss on iteration  639 209.04579701138633\n","Training Loss on iteration  640 209.04579698677372\n","Training Loss on iteration  641 209.04579696217812\n","Training Loss on iteration  642 209.04579693759953\n","Training Loss on iteration  643 209.04579691303795\n","Training Loss on iteration  644 209.04579688849336\n","Training Loss on iteration  645 209.04579686396576\n","Training Loss on iteration  646 209.0457968394551\n","Training Loss on iteration  647 209.04579681496142\n","Training Loss on iteration  648 209.04579679048467\n","Training Loss on iteration  649 209.04579676602484\n","Training Loss on iteration  650 209.04579674158197\n","Training Loss on iteration  651 209.04579671715598\n","Training Loss on iteration  652 209.0457966927469\n","Training Loss on iteration  653 209.04579666835468\n","Training Loss on iteration  654 209.04579664397937\n","Training Loss on iteration  655 209.0457966196209\n","Training Loss on iteration  656 209.04579659527928\n","Training Loss on iteration  657 209.0457965709545\n","Training Loss on iteration  658 209.04579654664653\n","Training Loss on iteration  659 209.04579652235543\n","Training Loss on iteration  660 209.04579649808107\n","Training Loss on iteration  661 209.04579647382354\n","Training Loss on iteration  662 209.04579644958278\n","Training Loss on iteration  663 209.04579642535876\n","Training Loss on iteration  664 209.04579640115153\n","Training Loss on iteration  665 209.04579637696105\n","Training Loss on iteration  666 209.04579635278725\n","Training Loss on iteration  667 209.04579632863025\n","Training Loss on iteration  668 209.04579630448995\n","Training Loss on iteration  669 209.0457962803663\n","Training Loss on iteration  670 209.0457962562594\n","Training Loss on iteration  671 209.04579623216912\n","Training Loss on iteration  672 209.04579620809548\n","Training Loss on iteration  673 209.04579618403855\n","Training Loss on iteration  674 209.04579615999825\n","Training Loss on iteration  675 209.0457961359746\n","Training Loss on iteration  676 209.04579611196755\n","Training Loss on iteration  677 209.04579608797712\n","Training Loss on iteration  678 209.04579606400324\n","Training Loss on iteration  679 209.04579604004593\n","Training Loss on iteration  680 209.04579601610524\n","Training Loss on iteration  681 209.04579599218113\n","Training Loss on iteration  682 209.04579596827352\n","Training Loss on iteration  683 209.0457959443825\n","Training Loss on iteration  684 209.04579592050797\n","Training Loss on iteration  685 209.04579589664996\n","Training Loss on iteration  686 209.04579587280844\n","Training Loss on iteration  687 209.04579584898343\n","Training Loss on iteration  688 209.0457958251749\n","Training Loss on iteration  689 209.04579580138287\n","Training Loss on iteration  690 209.04579577760722\n","Training Loss on iteration  691 209.04579575384807\n","Training Loss on iteration  692 209.04579573010537\n","Training Loss on iteration  693 209.04579570637907\n","Training Loss on iteration  694 209.0457956826692\n","Training Loss on iteration  695 209.04579565897572\n","Training Loss on iteration  696 209.04579563529862\n","Training Loss on iteration  697 209.04579561163789\n","Training Loss on iteration  698 209.04579558799352\n","Training Loss on iteration  699 209.04579556436556\n","Training Loss on iteration  700 209.04579554075391\n","Training Loss on iteration  701 209.0457955171586\n","Training Loss on iteration  702 209.04579549357962\n","Training Loss on iteration  703 209.04579547001697\n","Training Loss on iteration  704 209.04579544647055\n","Training Loss on iteration  705 209.04579542294047\n","Training Loss on iteration  706 209.04579539942665\n","Training Loss on iteration  707 209.0457953759291\n","Training Loss on iteration  708 209.0457953524478\n","Training Loss on iteration  709 209.04579532898273\n","Training Loss on iteration  710 209.04579530553394\n","Training Loss on iteration  711 209.04579528210132\n","Training Loss on iteration  712 209.04579525868493\n","Training Loss on iteration  713 209.04579523528474\n","Training Loss on iteration  714 209.04579521190072\n","Training Loss on iteration  715 209.0457951885329\n","Training Loss on iteration  716 209.04579516518123\n","Training Loss on iteration  717 209.04579514184573\n","Training Loss on iteration  718 209.04579511852637\n","Training Loss on iteration  719 209.0457950952231\n","Training Loss on iteration  720 209.045795071936\n","Training Loss on iteration  721 209.04579504866498\n","Training Loss on iteration  722 209.0457950254101\n","Training Loss on iteration  723 209.04579500217127\n","Training Loss on iteration  724 209.0457949789485\n","Training Loss on iteration  725 209.04579495574185\n","Training Loss on iteration  726 209.04579493255122\n","Training Loss on iteration  727 209.04579490937658\n","Training Loss on iteration  728 209.04579488621806\n","Training Loss on iteration  729 209.04579486307549\n","Training Loss on iteration  730 209.04579483994897\n","Training Loss on iteration  731 209.04579481683845\n","Training Loss on iteration  732 209.0457947937439\n","Training Loss on iteration  733 209.0457947706653\n","Training Loss on iteration  734 209.0457947476027\n","Training Loss on iteration  735 209.0457947245561\n","Training Loss on iteration  736 209.04579470152535\n","Training Loss on iteration  737 209.04579467851062\n","Training Loss on iteration  738 209.04579465551174\n","Training Loss on iteration  739 209.04579463252878\n","Training Loss on iteration  740 209.04579460956174\n","Training Loss on iteration  741 209.04579458661058\n","Training Loss on iteration  742 209.04579456367532\n","Training Loss on iteration  743 209.04579454075588\n","Training Loss on iteration  744 209.04579451785233\n","Training Loss on iteration  745 209.0457944949646\n","Training Loss on iteration  746 209.0457944720927\n","Training Loss on iteration  747 209.04579444923664\n","Training Loss on iteration  748 209.04579442639638\n","Training Loss on iteration  749 209.0457944035719\n","Training Loss on iteration  750 209.04579438076325\n","Training Loss on iteration  751 209.04579435797032\n","Training Loss on iteration  752 209.04579433519322\n","Training Loss on iteration  753 209.04579431243187\n","Training Loss on iteration  754 209.04579428968626\n","Training Loss on iteration  755 209.04579426695634\n","Training Loss on iteration  756 209.0457942442422\n","Training Loss on iteration  757 209.04579422154373\n","Training Loss on iteration  758 209.04579419886096\n","Training Loss on iteration  759 209.04579417619394\n","Training Loss on iteration  760 209.04579415354252\n","Training Loss on iteration  761 209.0457941309068\n","Training Loss on iteration  762 209.04579410828677\n","Training Loss on iteration  763 209.04579408568236\n","Training Loss on iteration  764 209.0457940630936\n","Training Loss on iteration  765 209.04579404052046\n","Training Loss on iteration  766 209.04579401796292\n","Training Loss on iteration  767 209.04579399542098\n","Training Loss on iteration  768 209.04579397289467\n","Training Loss on iteration  769 209.04579395038397\n","Training Loss on iteration  770 209.04579392788875\n","Training Loss on iteration  771 209.04579390540917\n","Training Loss on iteration  772 209.04579388294505\n","Training Loss on iteration  773 209.04579386049653\n","Training Loss on iteration  774 209.04579383806353\n","Training Loss on iteration  775 209.04579381564605\n","Training Loss on iteration  776 209.0457937932441\n","Training Loss on iteration  777 209.04579377085764\n","Training Loss on iteration  778 209.04579374848663\n","Training Loss on iteration  779 209.0457937261311\n","Training Loss on iteration  780 209.04579370379108\n","Training Loss on iteration  781 209.04579368146645\n","Training Loss on iteration  782 209.04579365915734\n","Training Loss on iteration  783 209.04579363686358\n","Training Loss on iteration  784 209.04579361458528\n","Training Loss on iteration  785 209.0457935923224\n","Training Loss on iteration  786 209.04579357007492\n","Training Loss on iteration  787 209.04579354784283\n","Training Loss on iteration  788 209.04579352562612\n","Training Loss on iteration  789 209.04579350342476\n","Training Loss on iteration  790 209.04579348123877\n","Training Loss on iteration  791 209.04579345906808\n","Training Loss on iteration  792 209.04579343691282\n","Training Loss on iteration  793 209.04579341477282\n","Training Loss on iteration  794 209.04579339264816\n","Training Loss on iteration  795 209.0457933705388\n","Training Loss on iteration  796 209.0457933484447\n","Training Loss on iteration  797 209.04579332636595\n","Training Loss on iteration  798 209.04579330430244\n","Training Loss on iteration  799 209.04579328225415\n","Training Loss on iteration  800 209.0457932602212\n","Training Loss on iteration  801 209.0457932382034\n","Training Loss on iteration  802 209.04579321620088\n","Training Loss on iteration  803 209.0457931942136\n","Training Loss on iteration  804 209.04579317224147\n","Training Loss on iteration  805 209.0457931502846\n","Training Loss on iteration  806 209.04579312834286\n","Training Loss on iteration  807 209.04579310641634\n","Training Loss on iteration  808 209.04579308450502\n","Training Loss on iteration  809 209.04579306260877\n","Training Loss on iteration  810 209.04579304072772\n","Training Loss on iteration  811 209.04579301886184\n","Training Loss on iteration  812 209.045792997011\n","Training Loss on iteration  813 209.04579297517532\n","Training Loss on iteration  814 209.04579295335475\n","Training Loss on iteration  815 209.04579293154924\n","Training Loss on iteration  816 209.04579290975886\n","Training Loss on iteration  817 209.04579288798354\n","Training Loss on iteration  818 209.04579286622328\n","Training Loss on iteration  819 209.04579284447806\n","Training Loss on iteration  820 209.0457928227479\n","Training Loss on iteration  821 209.04579280103275\n","Training Loss on iteration  822 209.04579277933263\n","Training Loss on iteration  823 209.04579275764755\n","Training Loss on iteration  824 209.04579273597744\n","Training Loss on iteration  825 209.04579271432232\n","Training Loss on iteration  826 209.0457926926822\n","Training Loss on iteration  827 209.04579267105706\n","Training Loss on iteration  828 209.04579264944684\n","Training Loss on iteration  829 209.04579262785154\n","Training Loss on iteration  830 209.04579260627128\n","Training Loss on iteration  831 209.04579258470582\n","Training Loss on iteration  832 209.0457925631554\n","Training Loss on iteration  833 209.04579254161985\n","Training Loss on iteration  834 209.04579252009916\n","Training Loss on iteration  835 209.04579249859339\n","Training Loss on iteration  836 209.0457924771025\n","Training Loss on iteration  837 209.04579245562644\n","Training Loss on iteration  838 209.04579243416526\n","Training Loss on iteration  839 209.04579241271895\n","Training Loss on iteration  840 209.04579239128745\n","Training Loss on iteration  841 209.04579236987075\n","Training Loss on iteration  842 209.0457923484689\n","Training Loss on iteration  843 209.04579232708187\n","Training Loss on iteration  844 209.0457923057096\n","Training Loss on iteration  845 209.04579228435213\n","Training Loss on iteration  846 209.04579226300942\n","Training Loss on iteration  847 209.04579224168148\n","Training Loss on iteration  848 209.04579222036833\n","Training Loss on iteration  849 209.04579219906987\n","Training Loss on iteration  850 209.04579217778618\n","Training Loss on iteration  851 209.04579215651717\n","Training Loss on iteration  852 209.04579213526287\n","Training Loss on iteration  853 209.04579211402333\n","Training Loss on iteration  854 209.04579209279845\n","Training Loss on iteration  855 209.04579207158824\n","Training Loss on iteration  856 209.04579205039275\n","Training Loss on iteration  857 209.04579202921187\n","Training Loss on iteration  858 209.04579200804568\n","Training Loss on iteration  859 209.04579198689407\n","Training Loss on iteration  860 209.04579196575713\n","Training Loss on iteration  861 209.04579194463486\n","Training Loss on iteration  862 209.0457919235272\n","Training Loss on iteration  863 209.04579190243405\n","Training Loss on iteration  864 209.0457918813555\n","Training Loss on iteration  865 209.0457918602916\n","Training Loss on iteration  866 209.04579183924227\n","Training Loss on iteration  867 209.04579181820745\n","Training Loss on iteration  868 209.0457917971872\n","Training Loss on iteration  869 209.04579177618152\n","Training Loss on iteration  870 209.04579175519032\n","Training Loss on iteration  871 209.04579173421365\n","Training Loss on iteration  872 209.04579171325153\n","Training Loss on iteration  873 209.0457916923039\n","Training Loss on iteration  874 209.04579167137072\n","Training Loss on iteration  875 209.0457916504521\n","Training Loss on iteration  876 209.04579162954784\n","Training Loss on iteration  877 209.04579160865813\n","Training Loss on iteration  878 209.04579158778284\n","Training Loss on iteration  879 209.045791566922\n","Training Loss on iteration  880 209.04579154607558\n","Training Loss on iteration  881 209.04579152524357\n","Training Loss on iteration  882 209.045791504426\n","Training Loss on iteration  883 209.0457914836228\n","Training Loss on iteration  884 209.04579146283396\n","Training Loss on iteration  885 209.04579144205957\n","Training Loss on iteration  886 209.04579142129953\n","Training Loss on iteration  887 209.04579140055384\n","Training Loss on iteration  888 209.04579137982253\n","Training Loss on iteration  889 209.0457913591055\n","Training Loss on iteration  890 209.04579133840284\n","Training Loss on iteration  891 209.0457913177145\n","Training Loss on iteration  892 209.04579129704047\n","Training Loss on iteration  893 209.04579127638073\n","Training Loss on iteration  894 209.0457912557353\n","Training Loss on iteration  895 209.04579123510413\n","Training Loss on iteration  896 209.04579121448725\n","Training Loss on iteration  897 209.04579119388464\n","Training Loss on iteration  898 209.04579117329627\n","Training Loss on iteration  899 209.04579115272216\n","Training Loss on iteration  900 209.04579113216224\n","Training Loss on iteration  901 209.04579111161658\n","Training Loss on iteration  902 209.0457910910851\n","Training Loss on iteration  903 209.04579107056784\n","Training Loss on iteration  904 209.0457910500648\n","Training Loss on iteration  905 209.0457910295759\n","Training Loss on iteration  906 209.04579100910118\n","Training Loss on iteration  907 209.04579098864065\n","Training Loss on iteration  908 209.04579096819424\n","Training Loss on iteration  909 209.045790947762\n","Training Loss on iteration  910 209.0457909273439\n","Training Loss on iteration  911 209.04579090693986\n","Training Loss on iteration  912 209.04579088655\n","Training Loss on iteration  913 209.04579086617423\n","Training Loss on iteration  914 209.04579084581255\n","Training Loss on iteration  915 209.04579082546496\n","Training Loss on iteration  916 209.04579080513145\n","Training Loss on iteration  917 209.04579078481197\n","Training Loss on iteration  918 209.0457907645066\n","Training Loss on iteration  919 209.04579074421525\n","Training Loss on iteration  920 209.04579072393793\n","Training Loss on iteration  921 209.04579070367464\n","Training Loss on iteration  922 209.04579068342537\n","Training Loss on iteration  923 209.04579066319013\n","Training Loss on iteration  924 209.04579064296883\n","Training Loss on iteration  925 209.04579062276161\n","Training Loss on iteration  926 209.04579060256827\n","Training Loss on iteration  927 209.04579058238895\n","Training Loss on iteration  928 209.04579056222354\n","Training Loss on iteration  929 209.04579054207218\n","Training Loss on iteration  930 209.0457905219347\n","Training Loss on iteration  931 209.04579050181115\n","Training Loss on iteration  932 209.0457904817015\n","Training Loss on iteration  933 209.04579046160575\n","Training Loss on iteration  934 209.04579044152396\n","Training Loss on iteration  935 209.04579042145605\n","Training Loss on iteration  936 209.04579040140197\n","Training Loss on iteration  937 209.04579038136183\n","Training Loss on iteration  938 209.04579036133546\n","Training Loss on iteration  939 209.04579034132306\n","Training Loss on iteration  940 209.0457903213244\n","Training Loss on iteration  941 209.04579030133962\n","Training Loss on iteration  942 209.0457902813687\n","Training Loss on iteration  943 209.04579026141155\n","Training Loss on iteration  944 209.0457902414682\n","Training Loss on iteration  945 209.04579022153865\n","Training Loss on iteration  946 209.0457902016229\n","Training Loss on iteration  947 209.04579018172097\n","Training Loss on iteration  948 209.04579016183274\n","Training Loss on iteration  949 209.0457901419583\n","Training Loss on iteration  950 209.0457901220976\n","Training Loss on iteration  951 209.04579010225063\n","Training Loss on iteration  952 209.04579008241737\n","Training Loss on iteration  953 209.04579006259786\n","Training Loss on iteration  954 209.04579004279208\n","Training Loss on iteration  955 209.04579002299994\n","Training Loss on iteration  956 209.04579000322158\n","Training Loss on iteration  957 209.0457899834568\n","Training Loss on iteration  958 209.04578996370574\n","Training Loss on iteration  959 209.04578994396834\n","Training Loss on iteration  960 209.04578992424464\n","Training Loss on iteration  961 209.04578990453453\n","Training Loss on iteration  962 209.04578988483806\n","Training Loss on iteration  963 209.04578986515523\n","Training Loss on iteration  964 209.04578984548598\n","Training Loss on iteration  965 209.04578982583038\n","Training Loss on iteration  966 209.04578980618834\n","Training Loss on iteration  967 209.04578978655994\n","Training Loss on iteration  968 209.04578976694506\n","Training Loss on iteration  969 209.04578974734378\n","Training Loss on iteration  970 209.04578972775602\n","Training Loss on iteration  971 209.04578970818187\n","Training Loss on iteration  972 209.04578968862123\n","Training Loss on iteration  973 209.0457896690741\n","Training Loss on iteration  974 209.04578964954052\n","Training Loss on iteration  975 209.04578963002047\n","Training Loss on iteration  976 209.0457896105139\n","Training Loss on iteration  977 209.0457895910208\n","Training Loss on iteration  978 209.0457895715412\n","Training Loss on iteration  979 209.04578955207512\n","Training Loss on iteration  980 209.04578953262245\n","Training Loss on iteration  981 209.04578951318322\n","Training Loss on iteration  982 209.0457894937575\n","Training Loss on iteration  983 209.0457894743452\n","Training Loss on iteration  984 209.04578945494632\n","Training Loss on iteration  985 209.04578943556083\n","Training Loss on iteration  986 209.04578941618877\n","Training Loss on iteration  987 209.04578939683014\n","Training Loss on iteration  988 209.04578937748485\n","Training Loss on iteration  989 209.04578935815297\n","Training Loss on iteration  990 209.04578933883448\n","Training Loss on iteration  991 209.04578931952935\n","Training Loss on iteration  992 209.04578930023757\n","Training Loss on iteration  993 209.04578928095913\n","Training Loss on iteration  994 209.045789261694\n","Training Loss on iteration  995 209.04578924244223\n","Training Loss on iteration  996 209.04578922320377\n","Training Loss on iteration  997 209.0457892039786\n","Training Loss on iteration  998 209.04578918476673\n","Training Loss on iteration  999 209.04578916556818\n","Training Loss on iteration  1000 209.04578914638293\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vanvD93FV0_k","colab_type":"text"},"source":["### Observe values of Weight\n","- Print the updated values"]},{"cell_type":"code","metadata":{"id":"QSqpy4gtWaOD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"outputId":"e8971c96-7bbe-49e1-fbd5-b22a27496f0f","executionInfo":{"status":"ok","timestamp":1580293420026,"user_tz":-330,"elapsed":786,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}}},"source":["print(w)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[0.06309611]\n"," [0.06333528]\n"," [0.0629523 ]\n"," [0.06349774]], shape=(4, 1), dtype=float64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y9KpRupYUEwy"},"source":["### Observe values of Bias\n","- Print the updated values"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bhEWkGqHWohg","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"adf4c7b2-6d93-4d76-c98e-75c6c5c137ec","executionInfo":{"status":"ok","timestamp":1580293431225,"user_tz":-330,"elapsed":862,"user":{"displayName":"Jayshree viswanathan","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvBX_hZhh5WfkATSGGvq06RuBfkaDCZpEsK2rkxw=s64","userId":"09879764889358438352"}}},"source":["print(b)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["tf.Tensor([5.30819226], shape=(1,), dtype=float64)\n"],"name":"stdout"}]}]}